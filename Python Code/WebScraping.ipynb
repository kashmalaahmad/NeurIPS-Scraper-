import os
import time
import requests
from bs4 import BeautifulSoup
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.auth.transport.requests import Request
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil

BASE_URL = "https://papers.nips.cc"
SCOPES = ["https://www.googleapis.com/auth/drive.file"]
CREDENTIALS_FILE_PATH = r"D:\\pythonCodes\\credentials.json"
TOKENS_DIRECTORY_PATH = "tokens"
THREAD_POOL_SIZE = 10

def authenticate_drive():
    print("Authenticating Google Drive...")
    creds = None
    token_path = os.path.join(TOKENS_DIRECTORY_PATH, "token.json")
    
    if os.path.exists(token_path):
        creds = Credentials.from_authorized_user_file(token_path, SCOPES)
        print("Loaded existing token.")

    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            print("Refreshing expired token...")
            creds.refresh(Request())
        else:
            print("Fetching new token...")
            flow = InstalledAppFlow.from_client_secrets_file(CREDENTIALS_FILE_PATH, SCOPES)
            creds = flow.run_local_server(port=0)

        os.makedirs(TOKENS_DIRECTORY_PATH, exist_ok=True)
        with open(token_path, "w") as token:
            token.write(creds.to_json())
            print("Token saved.")

    return build("drive", "v3", credentials=creds)

def download_pdf(pdf_url):
    filename = pdf_url.split("/")[-1]
    print(f"Downloading: {pdf_url}")
    retries = 3
    for attempt in range(retries):
        try:
            response = requests.get(pdf_url, timeout=60, stream=True)
            if response.status_code == 200:
                with open(filename, "wb") as file:
                    for chunk in response.iter_content(1024):
                        file.write(chunk)
                response.close()
                print(f"Downloaded: {filename}")
                return filename
            else:
                print(f"Failed to download {pdf_url}, Status Code: {response.status_code}")
        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
            print(f"Attempt {attempt + 1} failed for {pdf_url}: {e}")
            time.sleep(2 ** attempt)
    print(f"Failed to download {pdf_url} after {retries} retries.")
    return None

def upload_to_drive(file_name):
    print(f"Uploading {file_name} to Google Drive...")
    try:
        drive_service = authenticate_drive()
        file_metadata = {'name': file_name}
        media = MediaFileUpload(file_name, mimetype='application/pdf')
        drive_service.files().create(media_body=media, body=file_metadata).execute()
        print(f"Uploaded {file_name} successfully.")
        time.sleep(2)
    except Exception as e:
        print(f"Error uploading {file_name}: {e}")
    finally:
        if os.path.exists(file_name):
            for _ in range(5):
                if not check_file_locked(file_name):
                    try:
                        os.remove(file_name)
                        print(f"Deleted {file_name} from local storage.")
                        return
                    except Exception as e:
                        print(f"Failed to delete {file_name}: {e}")
                time.sleep(2)
            print(f"Failed to delete {file_name} after multiple attempts.")

def process_paper(paper_page_url):
    print(f"Processing paper page: {paper_page_url}")
    try:
        pdf_link = BeautifulSoup(requests.get(paper_page_url).text, 'html.parser').select_one(
            "a[href$='-Paper-Conference.pdf'], a[href$='-Paper.pdf']")
        if pdf_link:
            pdf_url = BASE_URL + pdf_link.get('href')
            print(f"Found PDF: {pdf_url}")
            filename = download_pdf(pdf_url)
            if filename:
                upload_to_drive(filename)
    except Exception as e:
        print(f"Error processing {paper_page_url}: {e}")

def process_year(year_url):
    print(f"Processing year page: {year_url}")
    try:
        paper_links = BeautifulSoup(requests.get(year_url).text, 'html.parser').select("a[href$='.html']")
        with ThreadPoolExecutor(max_workers=THREAD_POOL_SIZE) as executor:
            futures = [executor.submit(process_paper, BASE_URL + link.get('href')) for link in paper_links]
            for future in as_completed(futures):
                future.result()
    except Exception as e:
        print(f"Error processing year {year_url}: {e}")

def extract_year_from_url(url):
    try:
        return int(''.join(filter(str.isdigit, url)))
    except ValueError:
        return 0

def check_file_locked(file_name):
    for proc in psutil.process_iter(['pid', 'name']):
        try:
            for file in proc.open_files():
                if file.path == file_name:
                    return True
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
    return False

def main():
    print("Starting NeurIPS paper scraper...")
    try:
        authenticate_drive()
        year_urls = sorted(
            set([BASE_URL + link.get('href') for link in BeautifulSoup(requests.get(BASE_URL).text, 'html.parser').select("a[href^='/paper']")]),
            key=extract_year_from_url, reverse=True
        )
        latest_year_urls = year_urls[:5]
        print(f"Processing latest {len(latest_year_urls)} years: {latest_year_urls}")
        with ThreadPoolExecutor(max_workers=THREAD_POOL_SIZE) as executor:
            futures = [executor.submit(process_year, url) for url in latest_year_urls]
            for future in as_completed(futures):
                future.result()
    except Exception as e:
        print(f"Error in main execution: {e}")

if __name__ == "__main__":
    start_time = time.time()
    main()
    print(f"Total execution time: {time.time() - start_time:.2f} seconds")
